import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin

def fetch_page(url, headers):
    resp = requests.get(url, headers=headers)
    resp.raise_for_status()
    resp.encoding = "gbk"  # wenku8 ä½¿ç”¨ GBK ç·¨ç¢¼
    return resp.text

def parse_text(html):
    soup = BeautifulSoup(html, "html.parser")
    content_div = soup.find("div", id="content")
    if not content_div:
        return ""
    # å»é™¤å¤šé¤˜ç©ºç™½ä¸¦ä¿ç•™æ›è¡Œ
    text = content_div.get_text("\n", strip=True)
    return text

def find_next_page_url(html, base_url):
    soup = BeautifulSoup(html, "html.parser")
    next_link = soup.find("a", string=lambda s: s and "ä¸‹ä¸€é¡µ" in s)
    if next_link and next_link.get("href"):
        return urljoin(base_url, next_link["href"])
    return None

def scrape_all(start_url, output_filename):
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        )
    }

    url = start_url
    with open(output_filename, "w", encoding="utf-8") as f:
        while url:
            print(f"æŠ“å–ä¸­: {url}")
            try:
                html = fetch_page(url, headers)
            except Exception as e:
                print(f"âš ï¸ ç„¡æ³•è®€å– {url}: {e}")
                break

            text = parse_text(html)
            if not text.strip():
                print("âš ï¸ æ‰¾ä¸åˆ°æ­£æ–‡ï¼Œå¯èƒ½æ˜¯é˜²çˆ¬æˆ–é é¢æ ¼å¼ä¸åŒã€‚")
                break

            f.write(text)
            f.write("\n\n")

            next_url = find_next_page_url(html, url)
            if not next_url:
                print("âœ… æ²’æœ‰ä¸‹ä¸€é ï¼ŒæŠ“å–å®Œæˆã€‚")
                break

            url = next_url
            time.sleep(2)  # å»¶é²é¿å…è¢«å°

    print(f"\nğŸ“„ æ‰€æœ‰å…§å®¹å·²å„²å­˜åˆ° {output_filename}")

if __name__ == "__main__":
    start_url = "https://www.wenku8.net/novel/2/2654/102261.htm"
    output_file = "wenku8_novel.txt"
    scrape_all(start_url, output_file)
