import argparse
import os
import re
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import NoSuchElementException
import time
import random
import datetime

output_dir = "wenku8_novel"
next_btn_text = "ä¸‹ä¸€é¡µ"


def get_novel_title(driver):
    """æŠ“å–å°èªªåç¨±ä½œç‚ºæª”åä¸¦åŠ ä¸Šæ™‚é–“"""
    try:
        linkleft = driver.find_element(By.ID, "linkleft")
        a_tags = linkleft.find_elements(By.TAG_NAME, "a")
        if len(a_tags) >= 3:
            title = a_tags[2].text.strip()
            title = re.sub(r'[\\/:"*?<>|]', "", title)
        else:
            title = "novel"
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M")
        return f"{title}_{timestamp}"
    except Exception:
        return f"novel_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}"


def find_content(driver, log, current_url, max_retry=3):
    """å®‰å…¨æŠ“å–æ­£æ–‡å…§å®¹ï¼ˆå« Cloudflare é‡è©¦ï¼‰"""
    for attempt in range(1, max_retry + 1):
        chapter_name = driver.title.split("-")[0].strip()
        try:
            # Cloudflare åµæ¸¬
            if "Access denied" in driver.title or "Cloudflare" in driver.page_source:
                print(f"âš ï¸ Cloudflare æ””æˆªï¼ˆç¬¬ {attempt} æ¬¡é‡è©¦ï¼‰")
                log.write(
                    f"âš ï¸ Cloudflare æ””æˆªï¼ˆç¬¬ {attempt} æ¬¡é‡è©¦ï¼‰ URL: {current_url}\n"
                )
                time.sleep(random.uniform(8, 15))
                driver.refresh()
                continue

            content_div = driver.find_element(By.ID, "content")
            text = content_div.text.strip()
            if not text:
                raise ValueError("æ‰¾ä¸åˆ°æ­£æ–‡å…§å®¹")
            return text

        except Exception as e:
            print(f"âš ï¸ æŠ“å–å¤±æ•—ï¼ˆç¬¬ {attempt} æ¬¡ï¼‰ï¼š{e}")
            log.write(f"âš ï¸ æŠ“å–å¤±æ•—ï¼ˆç¬¬ {attempt} æ¬¡ï¼‰ã€{chapter_name}ã€‘- {e}\n")
            time.sleep(random.uniform(1.5, 3))
            driver.refresh()

    return None


def scrape_all(start_url, start_page=None, end_page=None):
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_argument("--lang=zh-TW")
    chrome_options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/142.0.0.0 Safari/537.36"
    )

    driver = webdriver.Chrome(service=Service(), options=chrome_options)
    driver.get(start_url)
    time.sleep(random.uniform(1.5, 3))

    os.makedirs(output_dir, exist_ok=True)
    novel_title = get_novel_title(driver)
    output_filename = os.path.join(output_dir, f"{novel_title}.txt")
    log_filename = os.path.join(output_dir, f"{novel_title}_log.txt")
    print(f"ğŸ“„ è¼¸å‡ºæª”æ¡ˆ: {output_filename}")

    page_count = 1
    start_time = time.time()

    with open(output_filename, "w", encoding="utf-8") as f, open(
        log_filename, "a", encoding="utf-8"
    ) as log:
        f.writelines(f"ã€Š{novel_title}ã€‹")
        log.write(f"\n--- é–‹å§‹æŠ“å– {datetime.datetime.now()} ---\n")
        log.write(f"èµ·å§‹ç¶²å€: {start_url}\n\n")

        # å¦‚æœæŒ‡å®šèµ·å§‹é ï¼Œå…ˆè·³åˆ°è©²é 
        if start_page and start_page > 1:
            for current_page in range(2, start_page + 1):
                try:
                    driver.find_element(By.LINK_TEXT, next_btn_text).click()
                    page_count += 1
                    time.sleep(random.uniform(1.5, 3))
                except NoSuchElementException:
                    log.write(
                        f"âš ï¸ ç„¡æ³•åˆ°é”ç¬¬ {start_page} é ï¼Œå¾ç¬¬ {current_page-1} é é–‹å§‹\n"
                    )
                    break

        # é–‹å§‹æŠ“å–
        while True:
            current_url = driver.current_url

            text = find_content(driver, log, current_url)
            chapter_name = driver.title.split("-")[0].strip()

            if text:
                elapsed = time.time() - start_time
                print(f"ã€{chapter_name}ã€‘{current_url} | ç¶“é {int(elapsed)} ç§’")
                f.write(f"\n\nã€{chapter_name}ã€‘\n\n{text}")
            else:
                print(f"âŒ ç„¡æ³•å–å¾—ã€{chapter_name}ã€‘å…§å®¹ï¼Œç•¥éæ­¤é ã€‚")
                log.write(
                    f"âŒ ç„¡æ³•å–å¾—ã€{chapter_name}ã€‘å…§å®¹ï¼Œç•¥éæ­¤é ã€‚\nURL: {current_url}\n"
                )

            # æª¢æŸ¥æ˜¯å¦é”åˆ°çµæŸé 
            if end_page and page_count >= end_page:
                print(f"âœ… å·²é”æŒ‡å®šé æ•¸ï¼ˆ{end_page}ï¼‰ï¼ŒçµæŸã€‚")
                log.write(f"âœ… å·²é”æŒ‡å®šé æ•¸ï¼ˆ{end_page}ï¼‰ï¼ŒçµæŸã€‚\n")
                break

            # ä¸‹ä¸€é 
            try:
                driver.find_element(By.LINK_TEXT, next_btn_text).click()
                page_count += 1
                time.sleep(1.5)
                # time.sleep(random.uniform(1.5, 3))
            except NoSuchElementException:
                print("âœ… æ²’æœ‰ä¸‹ä¸€é ï¼Œå®Œæˆã€‚")
                log.write("âœ… æ²’æœ‰ä¸‹ä¸€é ï¼Œå®Œæˆã€‚\n")
                break

    driver.quit()
    print(f"\nğŸ“˜ å°èªªå…§å®¹ï¼š{output_filename}")
    print(f"ğŸ§¾ éŒ¯èª¤æ—¥èªŒï¼š{log_filename}")


if __name__ == "__main__":
    print("ğŸ“š è½»å°è¯´æ–‡åº“ çˆ¬èŸ²å•Ÿå‹•...")
    parser = argparse.ArgumentParser(description="è½»å°è¯´æ–‡åº“ çˆ¬èŸ²")
    parser.add_argument(
        "start_url",
        help="èµ·å§‹ç¶²å€ï¼Œä¾‹å¦‚ï¼šhttps://www.wenku8.net/novel/2/2654/102261.htm",
    )
    parser.add_argument(
        "pages", nargs="*", type=int, help="å¯é¸ï¼šèµ·å§‹é  çµæŸé ï¼Œä¾‹å¦‚ 3 10"
    )
    args = parser.parse_args()

    start_page = args.pages[0] if len(args.pages) >= 1 else None
    end_page = args.pages[1] if len(args.pages) >= 2 else None

    scrape_all(args.start_url, start_page, end_page)
